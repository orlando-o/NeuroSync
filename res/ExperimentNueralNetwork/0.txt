## Model Architecture:

The code implements a Multi-Layer Feedforward Neural Network (MLFN) with one hidden layer. 

**Structure:**

* **Input Layer:** 784 neurons (corresponding to the flattened 28x28 MNIST image pixels)
* **Hidden Layer:** 128 neurons with the 'ReLU' activation function.
* **Output Layer:** 10 neurons (for the 10 digit classification) with the 'Softmax' activation function (implied by the use of CrossEntropyLoss).

## Hyperparameters:

The following machine learning hyperparameters are extracted from the code:

* **Input Size:** 784 
* **Hidden Size:** 128
* **Number of Classes:** 10
* **Epochs:** 5
* **Batch Size:** 64
* **Learning Rate:** 0.001
* **Optimizer:** Adam
* **Loss Function:** Cross-Entropy Loss 
